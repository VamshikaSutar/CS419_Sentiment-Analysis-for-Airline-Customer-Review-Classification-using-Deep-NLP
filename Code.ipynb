{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCJ2aKu-GivO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Pf_nECdHM6s",
        "outputId": "55eb2c92-b763-49e4-d747-4a96aceb5486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68LhB-G1HkDW",
        "outputId": "2c781578-a919-4b5c-bfa0-7bd5345394b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/singapore_airlines_reviews.csv')"
      ],
      "metadata": {
        "id": "DqJ8RQzHJ0t4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "dId4the3KVVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for data cleaning and lemmatization\n",
        "def clean_and_lemmatize(text):\n",
        "    # Remove punctuations\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
        "\n",
        "    # Remove stopwords and non-alphabetic characters, and lemmatize\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha() and token not in stop_words]\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n"
      ],
      "metadata": {
        "id": "iccPQ0jNKY2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned_text'] = data['text'].apply(clean_and_lemmatize)"
      ],
      "metadata": {
        "id": "h47OT-7gKcw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be88LMKrLMkh",
        "outputId": "61473ff4-0aa3-4e4c-ef8d-45a2fc7fbb56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 published_date published_platform  rating    type  \\\n",
            "0     2024-03-12T14:41:14-04:00            Desktop       3  review   \n",
            "1     2024-03-11T19:39:13-04:00            Desktop       5  review   \n",
            "2     2024-03-11T12:20:23-04:00            Desktop       1  review   \n",
            "3     2024-03-11T07:12:27-04:00            Desktop       5  review   \n",
            "4     2024-03-10T05:34:18-04:00            Desktop       2  review   \n",
            "...                         ...                ...     ...     ...   \n",
            "9995  2018-08-06T03:48:21-04:00            Desktop       5  review   \n",
            "9996  2018-08-05T22:50:29-04:00             Mobile       5  review   \n",
            "9997  2018-08-05T22:47:06-04:00            Desktop       5  review   \n",
            "9998  2018-08-05T20:32:03-04:00            Desktop       4  review   \n",
            "9999  2018-08-05T20:19:51-04:00            Desktop       4  review   \n",
            "\n",
            "                                                   text  \\\n",
            "0     We used this airline to go from Singapore to L...   \n",
            "1     The service on Singapore Airlines Suites Class...   \n",
            "2     Booked, paid and received email confirmation f...   \n",
            "3     Best airline in the world, seats, food, servic...   \n",
            "4     Premium Economy Seating on Singapore Airlines ...   \n",
            "...                                                 ...   \n",
            "9995  First part done with Singapore Airlines - acce...   \n",
            "9996  And again a great Flight with Singapore Air. G...   \n",
            "9997  We flew business class from Frankfurt, via Sin...   \n",
            "9998  As always, the A380 aircraft was spotlessly pr...   \n",
            "9999  As always, Singapore Airlines has done it agai...   \n",
            "\n",
            "                                                  title  helpful_votes  \\\n",
            "0                                                    Ok              0   \n",
            "1     The service in Suites Class makes one feel lik...              0   \n",
            "2                            Donâ€™t give them your money              0   \n",
            "3                             Best Airline in the World              0   \n",
            "4     Premium Economy Seating on Singapore Airlines ...              0   \n",
            "...                                                 ...            ...   \n",
            "9995  Flew to NZ 1st half Singapore Airlines, 2nd ha...              1   \n",
            "9996                                       Best Airline              1   \n",
            "9997               Superb service on Singapore Airlines              1   \n",
            "9998  A Comfortable Fiight Spoiled by lack of adequa...              2   \n",
            "9999                           Delivered as expected :)              3   \n",
            "\n",
            "                                           cleaned_text  \n",
            "0     used airline go singapore london heathrow issu...  \n",
            "1     service singapore airline suite class nothing ...  \n",
            "2     booked paid received email confirmation extra ...  \n",
            "3     best airline world seat food service brilliant...  \n",
            "4     premium economy seating singapore airline narr...  \n",
            "...                                                 ...  \n",
            "9995  first part done singapore airline acceptable c...  \n",
            "9996  great flight singapore air great unique servic...  \n",
            "9997  flew business class frankfurt via singapore br...  \n",
            "9998  always aircraft spotlessly presented boarding ...  \n",
            "9999  always singapore airline done redeye flight se...  \n",
            "\n",
            "[10000 rows x 8 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['cleaned_text']\n",
        "y = data['rating']"
      ],
      "metadata": {
        "id": "MtNJUiD64hy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_words = 10000\n",
        "max_length = 100\n",
        "embedding_dim = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)"
      ],
      "metadata": {
        "id": "QFsPZeEWqs9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = pad_sequences(sequences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "KaeXAailq4QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "6SCnjd3Jq9cQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Model"
      ],
      "metadata": {
        "id": "nLR1-X8kxUfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_length),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.2),\n",
        "    Dense(6, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "prCN6ZgGrAm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "B-8lFzcyrEA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkNoEEZprHCg",
        "outputId": "a4a7b923-6dee-4cfd-bd0c-219cb45db504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - 30s 262ms/step - loss: 1.1722 - accuracy: 0.5720 - val_loss: 0.8764 - val_accuracy: 0.6594\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 24s 242ms/step - loss: 0.8125 - accuracy: 0.6681 - val_loss: 0.8507 - val_accuracy: 0.6631\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 22s 224ms/step - loss: 0.6503 - accuracy: 0.7409 - val_loss: 0.8746 - val_accuracy: 0.6631\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 25s 244ms/step - loss: 0.5362 - accuracy: 0.7922 - val_loss: 0.9722 - val_accuracy: 0.6513\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 25s 247ms/step - loss: 0.4437 - accuracy: 0.8359 - val_loss: 1.0385 - val_accuracy: 0.6319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQP_juP6r-qP",
        "outputId": "11a74bdf-f4c7-4df6-fbf3-4c5b7b9050a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 3s 34ms/step - loss: 0.9346 - accuracy: 0.6485\n",
            "Test Accuracy: 0.6485000252723694\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model"
      ],
      "metadata": {
        "id": "_8z9Q9DxxZkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = Sequential([\n",
        "    Embedding(max_words, embedding_dim, input_length=max_length),\n",
        "    Conv1D(128, 5, activation='relu'),\n",
        "    MaxPooling1D(pool_size=4),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(6, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "vFQa8DkytYnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "6zR3_42Hvvx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model1.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2, callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdnxfYG0wTkP",
        "outputId": "ef3d1df2-0764-495c-b180-f83fb01a6006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "100/100 [==============================] - 9s 84ms/step - loss: 1.2263 - accuracy: 0.5392 - val_loss: 0.9206 - val_accuracy: 0.6425\n",
            "Epoch 2/20\n",
            "100/100 [==============================] - 10s 96ms/step - loss: 0.8773 - accuracy: 0.6484 - val_loss: 0.8100 - val_accuracy: 0.6862\n",
            "Epoch 3/20\n",
            "100/100 [==============================] - 10s 100ms/step - loss: 0.6643 - accuracy: 0.7317 - val_loss: 0.8212 - val_accuracy: 0.6775\n",
            "Epoch 4/20\n",
            "100/100 [==============================] - 8s 79ms/step - loss: 0.4816 - accuracy: 0.8250 - val_loss: 0.9158 - val_accuracy: 0.6637\n",
            "Epoch 5/20\n",
            "100/100 [==============================] - 10s 99ms/step - loss: 0.3442 - accuracy: 0.8780 - val_loss: 1.1317 - val_accuracy: 0.6637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model1.evaluate(X_test, y_test)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PvtBWWNw7Bt",
        "outputId": "97fc56ac-935a-42ce-9e39-63a4345cbf8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "63/63 [==============================] - 1s 11ms/step - loss: 0.8771 - accuracy: 0.6520\n",
            "Test Accuracy: 0.6520000100135803\n"
          ]
        }
      ]
    }
  ]
}
